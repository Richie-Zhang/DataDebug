% anomaly issue: we want to compare the effect of making 30 (K) random choices to 
% systematic exploration of the space (as in, Sum_{j != i}{|effect(a_j)-effect(a_i)} / N).
% compare false positives (we have non-anomaly but by bad luck, we exchange with anomalous impacts),
% and false negatives (we have an anomaly but by bad luck, we exchange with anomalous impacts).

This section presents an analysis of several aspects of data
debugging: its asymptotic runtime, the effectiveness of its sampling
approach to measure impact, and a justification of the use of the
normal distribution to score and select outliers from the impacts.

\subsection{Asymptotic Analysis}

Data debugging operates in several phases: computing the dependence
graph, performing impact analysis, and then ranking impacts. Runtime
depends on the following parameters: the number of data items ($n$),
the number of formulas ($f$), and the number of inputs ($i$). In
spreadsheets, the number of inputs equals the number of data items,
but in other contexts like databases, inputs correspond to fields, so
$i \ll n$. Since each formula and data item must be examined at least
once to compute the dependence graph and to measure impact,
respectively, runtime for data debugging must be at least
$\Omega(n+f)$.

The cost of building the dependence graph varies depending on the
structure of the computation. It has a worst-case runtime of
$O((if)^2)$), quadratic in the total number of inputs and formulas,
because it is theoretically possible for each formula to depend on
every input and other formula. However, this kind of pathological
structure is atypical. Dependence graphs normally form a tree or a
forest of trees. In this case, the cost of constructing the dependence
graph becomes linear in the number of inputs and formulas, or
$O(i+f)$.

Impact analysis dominates the costs of constructing the dependence
graph and ranking impacts, since it requires recalculation of the
computations affected by changes in the data.

The impact analysis phase runs in $\Theta(n)$ time; that is, it is
asymptotically optimal with respect to the number of
recalculations. The impact analysis for each item requires a fixed
number of random selections of other items in the same range. Each
item thus triggers a constant number of recalculations, so the total
number of recalculations is linear in the number of data items.


\subsection{Sampling Effectiveness}

\punt{
\paragraph{Normal distribution of impacts.}

Justifies use of trimming (only reporting things that are two standard
deviations)--high confidence that they are outliers.  Average change,
independent (random), same distribution (don't exclude same cell
swap). Use 30 samples.

\paragraph{Probability of missing important data.}

30 samples. Odds of missing something important are vanishingly small.
Cite Feller. Limitations: threshold functions.

\paragraph{Limitations}

\begin{itemize}
\item {\bf Array formulas.}
\item {\bf Tables.}
\item {\bf Control flow / HLOOKUP / VLOOKUP.}
\item {\bf Outputs that change data type.}
\item {\bf Macros / side-effects.}
\end{itemize}
}


\subsection{Impact Outlier Analysis}

Once all impact scores have been computed, only those data whose
impacts cross some threshold of anomalousness should be
reported. We currently treat the impact scores as if they fit a normal
distribution, and only report data whose scores place them more
than two standard deviations away from the mean. In a normal
distribution, that is just under 0.05\% of the population; in other
words, this corresponds to a 95\% confidence level that these are
anomalies.

In the absence of knowledge of the distribution of impact scores, using
the normal distribution is in general a conservative approach. That is, it
is likely to report few false negatives, at the risk of introducing some false
positives. This fact derives from two particular characteristics of the
normal distribution: its low \emph{skewness} and \emph{kurtosis}.

The normal distribution has zero skewness, where skew is the
distribution around the mean; in other words, it is perfectly
symmetric. Any asymmetric distribution by definition has a greater
number of points either to the left or to the right of the mean. By
choosing outliers from the tails of the normal, \checkcell{} also
includes the skewed tails of any other distribution.

In addition, the normal has either low or zero kurtosis, which
indicates the peakedness of the curve and heaviness of the tail;
whether the normal has a kurtosis of 0 or 3 depends on the definition
of kurtosis. Counting outliers from the perspective of the normal
distribution is generally conservative, because it includes all
distributions with heavier tails (those with positive
kurtosis). Because distributions with negative kurtosis generally have
small tails, they also tend to have few outliers by definition. Thus,
failing to report outliers for distributions with negative kurtosis is
generally safe.

One exception worth noting and a limitation of this approach is
symmetric bimodal distributions, where the outlier values are not only
in the tail but also centered around the mean. Using the normal as a
reference would not uncover such outliers.

% we just need to report outliers in the impact. assuming the impacts
% are normal is a conservative approach: the normal has 0 skewness
% (skew = distribution around the mean -- normal is symmetric, so 0
% skew) and low (either 0 or 3) kurtosis, depending on your definition
% of kurtosis. Every non-normal distribution is by definition more skewed and most
% distributions have a higher kurtosis (heavier tails), so we may
% overmark outliers. We won't find outliers in distributions with
% negative kurtosis, but those are super weird (no tails -- they drop
% below the x-axis at some point on either side), so it's hard to
% argue that they have outliers at all.

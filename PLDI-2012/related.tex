% The formula here is: discuss related work in one facet; end with a contrast with the current work.

\paragraph{Data Cleaning.}
Most past work on locating or removing errors in data has focused
on \emph{data cleaning} (also known as \emph{data scrubbing}
and \emph{cleansing}) in database
systems~\cite{DBLP:journals/debu/RahmD00,han2006data}. Standard
approaches include statistical outlier analysis for removing noisy
data~\cite{1583581}, interpolation to fill in missing data (e.g., with
averages), and using cross-correlation with other tables to correct or
locate errors~\cite{Hernandez:1995:MPL:223784.223807}.

% Also: noise removal.

% Section~\ref{FIXME} shows that statistical outlier analysis often produces unacceptably large numbers of false positives. 

% SURVEY! http://www.dbis.informatik.hu-berlin.de/dbisold/research/bioinformatics/papers/data_cleansing.html

A number of approaches have been developed that allow data cleaning to
be expressed programmatically or applied interactively. Programmatic
approaches include AJAX, which expresses a data cleaning program as a
DAG of transformations from input to
output~\cite{Galhardas:2000:AED:342009.336568}. Data Auditor applies
rules and target relations entered by a
programmer~\cite{Golab:2010:DAE:1920841.1921060}. A similar
domain-specific approach has been employed for data streams to smooth
data temporally and isolate it spatially~\cite{1617508}. Potter's
Wheel, by Raman and Hellerstein, is an interactive tool that lets
users visualize and apply data cleansing
transformations~\cite{Raman:2001:PWI:645927.672045}. 
Luebbers et al. describe an interactive data mining approach based on
machine learning that builds decision trees from databases. It marks
deviations from derived logical rules (e.g., ``$\mbox{BRV} =
404 \Rightarrow \mbox{GBM} = 901$'') as errors to be examined by a
data quality engineer~\cite{Luebbers:2003:SDD:1315451.1315499}.

Unlike these approaches, data debugging operates entirely
automatically (without the need for programmer-supplied rules or
latent logical relations in data) by measuring the interaction of data
with the programs that operate on them.
 

\paragraph{Spreadsheet Errors.}
Spreadsheets have been one of the most prominent computer applications
since their creation in 1979.
 The most widely used spreadsheet application today is Microsoft
Excel. Excel includes rudimentary error detection including errors in
formula entry like division by zero, a reference to a non-existient
formula or cell, invalid numerical arguments, or accidental mixing of
text and numbers.
% http://office.microsoft.com/en-us/excel-help/find-and-correct-errors-in-formulas-HP010066255.aspx
Excel also checks for inconsistency with adjacent formulas and other
structural errors, which it highlights with a ``squiggly'' underline. In addition, Excel provides a formula auditor, which lets spreadsheet view dependencies flowing into and out of a particular formula.
% http://office.microsoft.com/en-us/excel-help/use-error-checking-to-correct-common-errors-in-formulas-HA010342331.aspx

Past work on detecting errors in spreadsheets has focused on inferring
units and relationships (has-a, is-a) from information like layout and
headers~\cite{DBLP:conf/kbse/AhmadAGK03}. XeLda checks if formulas
process values with incorrect units or if derived units clash with
unit annotations~\cite{Antoniu:2004:VUC:998675.999448}.  More type
system stuff:~\cite{Erwig:2005:AGM:1062455.1062494}. Using labels and
structural clues (especially unit-of-measurement
errors)~\cite{Chambers:2010:RSL:1860134.1860346}. There also has been
considerable work on testing tools for
spreadsheets~\cite{fisher2006scaling,rothermel1998you,rothermel2001methodology,Carver:2006:EET:1159733.1159775}). Much
of this work is complementary and orthogonal to \checkcell{}, which
works with standard, unannotated spreadsheets and focuses on unusual
interactions of data with formulas.


% What is Jaffry et al\. ~\cite{DBLP:journals/corr/abs-0803-1748}?

\paragraph{Outlier Analysis}

Techniques to remove outliers (unusual data) date to the earliest days
of statistics, when they were developed to make nautical measurements
more robust. Widely-used approaches include Peirce's criterion,
Chauvenet's criterion, and Grubb's test for outliers~\cite{barnett1994outliers}. All
of these techniques require that data belong to a known distribution,
primarily the normal (Gaussian). Unfortunately, input data does not
always fit any statistical distribution. Even when outlier detection is
possible, identifying them leads to false positives when they do not
materially contribute to the result of a computation.
\checkcell{} leverages the
fact that its statistical sampling approach forces observed effects to
follow a Gaussian distribution (as Section~\ref{sec:FIXME} explains),
allowing it to use the Peirce outlier test to determine whether a
particular value has an unusual impact on a computation.


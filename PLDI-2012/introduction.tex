% \textbf{Software bugs: well-studied, effective tools.}
In many computational tasks, correctness is a primary concern. Most
work in the programming language community has
focused on ways to discover whether the program performing the
computation is correct. Techniques to reduce program errors range from
testing and runtime assertions, to dynamic and static analysis tools
that can discover a wide range of bugs. Using these approaches and
tools greatly increases the ability of programmers to find errors and
reduce their impact, contributing to improving overall code quality.

% ~\cite{unittesting,Miller:1990:ESR:96267.96279}
% ~\cite{samanderansthing,others}
% ~\cite{valgrind,dawsonthing,otherpcmemberfoo}

% Data errors, not so much.
However, a program is just one part of a computation. A computation
consists of a program and its input. If the input contains errors, the
result of the computation is likely to not be correct. Unlike
programs, data cannot easily be tested or analyzed for correctness.

Input data
errors can arise in a variety of ways~\cite{hellerstein2008quantitative}:

\begin{itemize}

\item {\bf Data entry errors}, including typographical errors and transcription errors from illegible text.

\item {\bf Measurement errors}, when the data source itself, such as a disk or a sensor, is faulty or corrupted, or when the method of obtaining the data is faulty (e.g., miscalibration).

\item {\bf Data integration errors}, where inconsistencies arise due to the mixing of different data, including unit of measurement mismatches.

\end{itemize}

% \textbf{Data errors really important in data-intensive programming environments.}

While data errors pose a threat to the correctness of any computation,
they are especially problematic in data-intensive programming
environments like databases, spreadsheets, and certain scientific
computations. In these
settings, data correctness can be as important as program correctness
(``garbage in, garbage out''). The results produced by the
computations---queries, formulas, charts, and other analyses---can end
up being rendered invalid by data errors. These errors can be costly:
for example, spreadsheet errors have led to losses of millions of
dollars~\cite{DBLP:journals/corr/abs-0803-2527,sakalerrors}.

%%%%  Things to add in citations: %%%%
% TransAlta took a $24 million charge -- copy and paste error
% http://www.skillsportal.co.za/page/training/articles/512049-Spreadsheet-errors-can-be-a-major-cost-to-your-business#.UJbfX2l250s
% http://www.flintshirechronicle.co.uk/flintshire-news/local-flintshire-news/2010/02/18/flintshire-county-council-school-cash-blunder-down-to-spreadsheet-error-51352-25856321/
% http://binnenland.nieuws.nl/566978


% cite approximate computation stuff?

By contrast with the proliferation of tools at a programmer's disposal
to find program errors, few tools exist to help find data errors. Part
of the problem is that, unlike program errors, it is more difficult to
decide whether any given data element is an error or not. For example,
the number \texttt{1234} might be correct, or it could be a
typographical error when the correct value is \texttt{12.34}. Omitted
or transposed digits or decimal points can change a data item by
orders of magnitude. Unfortunately, manual data auditing to find this
kind of mistake is both onerous and does not scale.
% to even the moderate size of data in spreadsheets.


% \textbf{Existing approaches don't really work.}

Existing approaches to finding data errors include
\emph{data cleaning} and  \emph{statistical outlier detection}.
Data cleaning primarily copes with errors in databases via
cross-validation with ground truth data, which may not be
present. Statistical outlier detection typically reports data as
outliers based on their relationship to a given distribution (e.g.,
Gaussian).  Automatic identification of data distributions is
error-prone and can give rise to excessive false positives.

% Non-parametric outlier detection methods
%like clustering and kernel density estimators known distributions lack
%statistical \emph{power}: they have a high probability of missing
%outliers.

%, data debugging reframes the problem to find data
%whose presence has an unusually large impact on the computation as a
%whole.

\paragraph{Contributions:}
This paper presents \emph{data debugging}, an approach to locating
likely data errors.  Data debugging combines dataflow dependence
analysis and statistical analysis to isolate data with an
unusually high impact on the final results of a computation.

By calling attention to this data, data debugging can provide insights
into both the data and the computation and reveal errors. Since it is
impossible to know \emph{a priori} whether data are erroneous or not,
data debugging does the next best thing: \emph{locating data where an
error would have the most impact}. Intuitively, data that has an
inordinate impact on the final result is either very important or it
is wrong. By contrast, wrong data whose presence has little impact on
the final result does not merit special attention.

We believe data debugging is broadly applicable, though it is
especially well-suited for data-intensive programming settings
like spreadsheets, databases, and environments like
R~\cite{ihaka1996r}, where data and programs (e.g., queries and
formulas) are intertwined.

Data debugging works by first building a data dependence graph, and
then measuring the impact of replacing data items with randomly chosen
data from the same distribution (e.g., columns or ranges in databases
or spreadsheets) and observing the changes in any computation that
depends on that data. This non-parametric statistical approach allows
data debugging to find errors in both numeric and non-numeric data,
without any requirement that data follow any statistical distribution.

We present an instantiation of data debugging in the form
of \checkcell{}, an add-in for Microsoft Excel. Spreadsheets are one
of the most widely-used programming environments, and this domain has
recently attracted renewed academic
attention~\cite{DBLP:conf/popl/Gulwani11,DBLP:conf/pldi/HarrisG11,Singh:2012:LSS:2212351.2212356}.
\checkcell{} ranks by impact all data whose
impact crosses a threshold of unusualness (above the 95\% confidence
level).  In the user interface, \checkcell{} colors these cells in
shades proportionally to their impact: the brighter a cell is
highlighted, the greater impact it has.

\checkcell{} is efficient: it operates in time linear in the number
of data elements, which is optimal. The current prototype is untuned
but analysis time is generally low, taking less than a minute to run
on spreadsheets containing thousands of cells. A user study verifies
the hypothesis that data debugging's approach of identifying data with
unusual impacts is effective at locating errors. With \checkcell{}'s
help, users were able to find XX\% of injected errors, while users
without \checkcell{} were only able to find YY\% of errors.


%We have developed a Microsoft Excel extension, or add-in, written in
%C\#, which uses cross-validation techniques and perturbation analysis
%to look for numerical values that appear suspicious.  Values that
%appear to be potential errors are highlighted proportionally to the
%likeliness that they are incorrect, as judged by our analysis -- the
%more likely a value is to be wrong, the brighter the highlighting.

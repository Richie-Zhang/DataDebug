% \textbf{Software bugs: well-studied, effective tools.}
In many computational tasks, correctness is a primary concern. Most
work in the programming language community over the past decades has
focused on ways to discover whether the program performing the
computation is correct. Techniques to reduce program errors range from
testing and runtime assertions, to dynamic and static analysis tools
that can discover a wide range of bugs. Using these approaches and
tools greatly increases the ability of programmers to find errors and
reduce their impact, contributing to improving overall code quality.

% ~\cite{unittesting,Miller:1990:ESR:96267.96279}
% ~\cite{samanderansthing,others}
% ~\cite{valgrind,dawsonthing,otherpcmemberfoo}

% Data errors, not so much.
However, a program is just one part of a computation. A computation
consists of a program and its input. If the input contains errors, the
result of the computation is likely to not be correct. Unlike
programs, data cannot easily be tested or analyzed for correctness.

Input data
errors can arise in a number of ways~\cite{hellerstein2008quantitative}.

\begin{itemize}

\item {\bf Data entry errors}, including typographical errors and transcription errors from illegible text.

\item {\bf Measurement errors}, when the data source itself, such as a network connection, a disk, or a sensor, is faulty or corrupted, or when the method of obtaining the data is faulty (e.g., miscalibration or misplacement of a sensor).

\item {\bf Data integration errors}, where inconsistencies arise due to the mixing of different data, including unit of measurement mismatches.

\end{itemize}

% \textbf{Data errors really important in data-intensive programming environments.}

While data errors pose a threat to the correctness of any computation,
they are especially problematic in data-intensive programming
environments like databases, spreadsheets, and certain scientific
computations. In these
settings, data correctness can be as important as program correctness
(``garbage in, garbage out''). The results produced by the
computations---queries, formulas, charts, and other analyses---can end
up being rendered invalid by data errors. These errors can be costly:
for example, spreadsheet errors have led to losses of millions of
dollars. % ~\cite{FIXME}.

%%%%  Things to add in citations: %%%%
% TransAlta took a $24 million charge -- copy and paste error
% http://www.skillsportal.co.za/page/training/articles/512049-Spreadsheet-errors-can-be-a-major-cost-to-your-business#.UJbfX2l250s
% http://www.flintshirechronicle.co.uk/flintshire-news/local-flintshire-news/2010/02/18/flintshire-county-council-school-cash-blunder-down-to-spreadsheet-error-51352-25856321/
% http://binnenland.nieuws.nl/566978


% cite approximate computation stuff?

By contrast with the proliferation of tools at a programmer's disposal
to find program errors, few tools exist to help find data errors. Part
of the problem is that, unlike program errors, it is more difficult to
decide whether any given data element is an error or not. For example,
the number \texttt{132} might be correct, or it could be a
transposition error from \texttt{123}. More insidiously, a misplaced
or omitted decimal point could change a data item by orders of
magnitude. Unfortunately, manual data auditing to find this kind of
mistake is both onerous and difficult to scale up.
% to even the moderate size of data in spreadsheets.


% \textbf{Existing approaches don't really work.}

Existing approaches to finding data errors include
statistical \emph{outlier detection} and \emph{data cleaning}. Outlier
detection methods typically require a known distribution (e.g.,
Gaussian). Automatic identification of data distributions is
error-prone and can give rise to an excessive number of false
positives. Non-parametric outlier detection methods that do not depend
on known distributions lack statistical \emph{power}: they have a high
probability of missing outliers. Data cleaning primarily copes with
errors in databases via cross-validation with ground truth data, which
may not be present.

\subsection*{Contributions}

%, data debugging reframes the problem to find data
%whose presence has an unusually large impact on the computation as a
%whole.

This paper presents \emph{data debugging}, an approach to locating
likely data errors.  Data debugging combines dataflow dependence
analysis and statistical analysis to isolate data with an
unusually high impact on the final results of a computation.

By calling attention to this data, data debugging can provide insights
into both the data and the computation and reveal errors. Since it is
impossible to know \emph{a priori} whether data are erroneous or not,
data debugging does the next best thing: \emph{locating data where an
error would have the most impact}. Intuitively, data that has an
inordinate impact on the final result is either very important or it
is wrong. By contrast, data that is wrong but whose inclusion or
exclusion has little impact on the final result does not merit special
attention.

We believe data debugging is broadly applicable, though it is
especially well-suited for data-intensive programming environments
like spreadsheets, databases, and interactive data analysis tools like
R~\cite{ihaka1996r}, where data and programs (e.g., queries and
formulas) are intertwined.

We present an instantiation of data debugging in the form
of \checkcell{}, an add-in for Microsoft Excel. Spreadsheets are one
of the most widely-used programming environments, and this domain has
recently attracted renewed academic
attention~\cite{DBLP:conf/popl/Gulwani11,DBLP:conf/pldi/HarrisG11,Singh:2012:LSS:2212351.2212356}.

A user who wants to check for data errors can click on
the \checkcell{} button. \checkcell{} builds a dataflow graph of the
entire computation that the spreadsheet represents, where outputs and
intermediate nodes include formulas and charts, and where data cells
and ranges of cells form the leaves. It then systematically measures
the impact of replacing each cell with randomly chosen cells within
the same range by observing the change in a resulting recalculation of
the spreadsheet. This non-parametric statistical approach
allows \checkcell{} to find errors in distribution-free numeric and
non-numeric data. \checkcell{} then ranks by impact all data whose
effect crosses a threshold of unusualness (above the 95\% confidence
level).  In the user interface, \checkcell{} colors these cells in
shades proportionally to their impact: the brighter
a cell is highlighted, the greater impact it has.

\checkcell{} is efficient: it operates in time linear in the number
of data elements, which is optimal. The current prototype is untuned
but analysis time is generally low, taking less than a minute to run
on spreadsheets containing thousands of cells. A user study verifies
the hypothesis that data debugging's approach of identifying data with
unusual impacts is effective at locating errors. With \checkcell{}'s
help, users were able to find XX\% of injected errors, while users
without \checkcell{} were only able to find YY\% of errors.


%We have developed a Microsoft Excel extension, or add-in, written in
%C\#, which uses cross-validation techniques and perturbation analysis
%to look for numerical values that appear suspicious.  Values that
%appear to be potential errors are highlighted proportionally to the
%likeliness that they are incorrect, as judged by our analysis -- the
%more likely a value is to be wrong, the brighter the highlighting.

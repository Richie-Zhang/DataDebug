At start, we have a spreadsheet with some values and some formulae.

The graph we build in step #2 is a dataflow DAG (D), therefore, the inputs are the parents, and the outputs are the children of those parents.

# Find formulae
1. Find all formulae in the spreadsheet, store in a list.

# Build dataflow graph
2. Parse each formulae to determine:
  A. whether this formula breaks homogeneity assumption (e.g., HLOOKUP)
  B. inputs
  C. check in DAG D whether a node for the formula exists. if not, create it, then:
    i. in the case that a parent does not exist (it could be another cell or a range of cells), we create the node for the parent as well. anytime we encounter a range, add it to a list of ranges R (at most once).
    ii. link to each parent, and for each parent link back to this node (because this node is a child of that parent)

# repeat DAG construction process for charts
# we have formulae for each kind of chart representing their meaning

# Find outputs
3. Find all nodes in D which have parents but no children.  These are outputs by definition, either formulae or charts.  Save in list O.  Also save original values of these outputs in list OV.

# Create a normalized average impact matrix, I
4. make matrix with the following two dimensions: cell address, index in O
  A. initialize all of the values in I to zero.
  
# Create a matrix of min and max outputs, M
5. make matrix with the following two dimensions: index in O, min or max
  each entry in the matrix will store the greatest or smallest impact value seen for each output o.
  A. initialize every M[o][max] = 0
  B. initialize every M[o][min] = +\infty

# Create a list of swap counts for each input cell, S
6. make list, indexed by cell address
  A. initialize every value in S to 0.

# Do perturbation
7. For each range r in R,
 A. For each cell c in r,
  i. Let c_orig = c
  ii. Let c_siblings = a random sample (without replacement) of size 30 of the cells in r unless |r| < 30, in which case, take all of r.
  ii. For each c' in c_siblings,
    a. replace c with c', and then, for each output o in O:
      I. I[c.address][o] = abs(O[o].value - OV[o])
        This keeps track of each cell's AVERAGE impact on each output
      II. if I[c.address][o] > M[o][max] then M[o][max] = I[c.address][o]
          i.e., keep track of the max
      III. if I[c.address][o] < M[o][min] then M[o][min] = I[c.address][o]
          i.e., keep track of the min

# I now represents the per-swap average impact
8. For each cell c in I,
  A. For each output o in I[c],
    i. I[c][o] = I[c][o] / S[c] # i.e., find the average value

# Normalize I so that all of the values for a particular output fall
# between 0 and 1 so that no output is deemed more important than another
9. For each cell c in I,
  A. for each output o in O,
    i. I[c][o] = (I[c][o] - M[o][min]) / (M[o][max] - M[o][min])
      i.e., normalize every value
      
# Create a matrix, Z, which stores the z-score of each cell's normalized average impact on a particular output
10. Make Z: cell address, index in O
  A. Init to zero

# compute z-scores for every 
11. For each o in O,
  let o_sum = 0
  A. For each c in I,
    i. o_sum += I[c][o]
  B. o_mean = o_sum / length(I[0])
  C. o_var = 0
  D. For each c in I,
    i. o_var += (o_mean - I[c][o])^2) / length(I[0])
  E. o_sigma = sqrt(o_var)
  F. For each c in I,
    i. Z[c][o] = abs(o_mean - I[c][o]) / o_sigma # compute z-score

# this is for storing the average z-score for each cell
12. Create a new list, $\mu_z$ indexed by c

# now we compute the average z-score for each cell across all outputs
# this is computed so that we can find a total order on outliers, globally
13. For each c in Z,
  z_sum = 0
  A. For each o in Z[c],
    z_sum += Z[c][o]
  B. $\mu_z$[c] = z_sum / length(Z[c])
  
# Now eliminate outliers, using Peirce (or any other outlier rejection procedure for the normal distribution that you want)
14. Run Peirce($\mu_z$).  Flag all rejected values.
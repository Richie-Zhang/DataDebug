\section{Introduction}
\label{sec:introduction}

% \textbf{Software bugs: well-studied, effective tools.}
Correctness is a key
concern for nearly all computations. Many tools exist to combat program errors, ranging from testing and
runtime assertions, to dynamic and static analysis tools. 
These tools and approaches enable programmers to find errors and
reduce their impact, contributing to improving overall code quality.

% ~\cite{unittesting,Miller:1990:ESR:96267.96279}
% ~\cite{samanderansthing,others}
% ~\cite{valgrind,dawsonthing,otherpcmemberfoo}

% Data errors, not so much.
However, the program is just one part of a computation. Existing tools ignore the correctness of program \emph{inputs}. If the input contains errors, the
result of the computation is likely to not be correct. Unlike
programs, data cannot easily be tested or analyzed for correctness.

Input data
errors can arise in a variety of ways~\cite{hellerstein2008quantitative}:

\begin{itemize}

\item {\bf Data entry errors}, including typographical errors and transcription errors from illegible text.

\item {\bf Measurement errors}, when the data source itself, such as a disk or a sensor, is faulty or corrupted (unintentionally or not).

\item {\bf Data integration errors}, where inconsistencies arise due to the mixing of different data, including unit of measurement mismatches.

\end{itemize}

% \textbf{Data errors really important in data-intensive programming environments.}

While data errors pose a threat to the correctness of any computation,
they are especially problematic in data-intensive programming
environments like databases, spreadsheets, and certain scientific
computations. In these settings, data correctness can be as important
as program correctness (``garbage in, garbage out''). The results
produced by the computations---queries, formulas, charts, and other
analyses---may be rendered invalid by data errors. These errors can be
costly: errors in spreadsheet data have led to losses of millions of
dollars~\cite{DBLP:journals/corr/abs-0803-2527,sakalerrors}, and poor
data quality has been estimated to cost the US economy more than \$600
billion per year~\cite{eckerson2002}.

%%%%  Things to add in citations: %%%%
% TransAlta took a $24 million charge -- copy and paste error
% http://www.skillsportal.co.za/page/training/articles/512049-Spreadsheet-errors-can-be-a-major-cost-to-your-business#.UJbfX2l250s
% http://www.flintshirechronicle.co.uk/flintshire-news/local-flintshire-news/2010/02/18/flintshire-county-council-school-cash-blunder-down-to-spreadsheet-error-51352-25856321/
% http://binnenland.nieuws.nl/566978


% cite approximate computation stuff?

By contrast with the proliferation of tools at a programmer's disposal
to find program errors, few tools exist to help find data errors. Part
of the problem is that it can be difficult to decide whether any given
data element is an error or not. For example, the number \texttt{1234}
might be correct, or the correct value might
be \texttt{12.34}. Typographical errors can change data items by
orders of magnitude. Unfortunately, finding this kind of mistake via
manual data auditing is onerous, unscalable, and error-prone.

% to even the moderate size of data in spreadsheets.


% \textbf{Existing approaches don't really work.}

Existing approaches to finding data errors include
\emph{data cleaning} and  \emph{statistical outlier detection}.
Data cleaning primarily copes with errors via
cross-validation with ground truth data, which may not be
present, or with rules, which do not adapt to data changes and quickly become complex and unmanageable. Statistical outlier detection typically reports data as
outliers based on their relationship to a given distribution (e.g.,
Gaussian).  Automatic identification of data distributions is
error-prone and can give rise to excessive false positives.

% Non-parametric outlier detection methods
%like clustering and kernel density estimators known distributions lack
%statistical \emph{power}: they have a high probability of missing
%outliers.

%, data debugging reframes the problem to find data
%whose presence has an unusually large impact on the computation as a
%whole.

This project proposes \emph{\bf data debugging}, an approach for locating
potential data errors. Since it is impossible to know \emph{a priori}
whether data are erroneous or not, data debugging does the next best
thing: \emph{locate data that has an unusual
impact on the computation}. Intuitively, data that has an inordinate impact on the final
result is either very important, or it is wrong. By contrast, wrong
data whose presence has no particularly unusual effect on the final result does not
merit special attention.  Data debugging combines data dependence
analysis and statistical analysis to find and rank data based on the unusualness of its
impact on the results of a computation.

Data debugging works by first building a data dependence graph of the
computations. It then measures data impact by replacing data items
with data chosen from the same group (e.g., a range in a spreadsheet
formula) and observing the resulting changes in computations that
depend on that data. This non-parametric approach allows data
debugging to find errors in both numeric and non-numeric data, without
any requirement that data follow any particular statistical
distribution.

By calling attention to data with unusual impact, data debugging can
provide insights into both the data and the computation and reveal
errors. We believe data debugging is broadly applicable, though it is
especially well-suited for data-intensive programming environments that intertwine
data and programs (e.g., with queries and formulas).

We have developed a prototype of a data debugging tool in the form
of \checkcell{}, an add-in for Microsoft Excel. Spreadsheets are one
of the most widely-used programming environments, and this domain has
recently attracted renewed academic
attention~\cite{DBLP:conf/popl/Gulwani11,DBLP:conf/pldi/HarrisG11,Singh:2012:LSS:2212351.2212356}.
In addition, spreadsheet errors are a well known risk and have led to
significant monetary losses in the past, making them an excellent
first target for data debugging.

\checkcell{} highlights all data whose impact crosses a threshold of
unusualness that is more than two standard deviations away from the
mean impact.  In the user interface, \checkcell{} ranks these cells by
coloring them in shades proportionally to their impact: the brighter a
cell is highlighted, the more unusual impact it has. \checkcell{} is
empirically and analytically efficient and effective, as we show in
Section~\ref{sec:evaluation}. The current prototype is untuned but
analysis time is generally low, taking seconds to run on most of the
spreadsheets we examine (although occasionally taking minutes). We
perform a case study by employing human workers via a crowdsourcing
platform (Amazon's Mechanical Turk), and show that \checkcell{} is
effective at finding actual data entry errors.

\subsection*{Overview and Contributions of Proposed Work}

This project will develop data debugging in several key directions,
improving its performance and broadening its scope. We will develop
data debugging approaches that apply at different scales:

\paragraph{Small-scale: Spreadsheets.} We plan to apply static
analysis to formulas to enable optimizations and minimize
recomputations when data changes, making it possible to detect
likely errors as soon as they are entered.

\paragraph{Medium-scale: Database systems.} We will extend data debugging to
relational database systems by combining static and dynamic analysis
in an approach we call \emph{bi-directional
  analysis} to efficiently identify erroneous tuples and trace errors back to their sources.

\paragraph{Large-scale: MapReduce.} At the largest scale, we will incorporate data debugging
into MapReduce tasks. Because human review of possible errors at this
scale is intractable, we will apply data debugging techniques both to
identify erroneous data sources and to allow MapReduce tasks to
withstand errors automatically.

Table~\ref{tab:data-debug-plan} presents an overview of our prior,
preliminary, and proposed work, and a breakdown of responsibility for
each task by PI. PI Berger will lead the effort on applying data
debugging to spreadsheets, while PI Meliou will lead the effort for
databases. Both will jointly lead the effort on applying data
debugging to MapReduce. The PIs have had several meetings separately
and together with graduate students in planning for this grant, and we
plan to have weekly data debugging meetings of PIs and graduate
students once the grant is funded.

\begin{table}[!t]
\centering
\begin{tabular}{llcccc}
\textbf{Domain} & \textbf{Task} & \textbf{Prior} & \textbf{Preliminary} & \textbf{Proposed} & \textbf{PIs} \\
\hline
\multicolumn{6}{c}{\emph{small-scale}} \\
Spreadsheets               & \checkcell{} implementation       & & \checkmark &  & Berger \\
                           & static analysis optimizations     & &  & \checkmark & Berger \\
\hline
\multicolumn{6}{c}{\emph{medium-scale}} \\
Databases                  & optimized forward analysis  & & & \checkmark          & Meliou \\
                           & reverse analysis & \checkmark & & \checkmark & Meliou \\
                           & (tracking error sources) & & & & \\
% ~\cite{DBLP:conf/sigmod/MeliouS12, DBLP:conf/sigmod/MeliouSS12}
                           & bi-directional analysis & & & \checkmark & Meliou \\
\hline
\multicolumn{6}{c}{\emph{large-scale}} \\
MapReduce                  & input trimming  & & & \checkmark & Berger, Meliou\\
                           & (withstanding errors) & & & & \\
\hline
\end{tabular}
\caption{Data Debugging: Prior, Preliminary, Proposed Work, and Associated PIs. \comment{scales look confusing. I like them in the table but their positioning throws me off.}\label{tab:data-debug-plan}}
\end{table}

\paragraph{Broader Impact.} 
Data debugging is a new paradigm for attacking the problem of errors
in data by leveraging the interaction between data and the programs
that operate on them.

\paragraph{Scientific community} % (fold)
\label{par:scientific}
Scientists in a variety of fields are accumulating and curating shared data repositories~\cite{uniprot,naturemap}. Several disciplines also combine their data into large-scale repositories. These areas include astrophysics~\cite{skysurvey}, earth science~\cite{iris,unavco}, coastal and environmental science~\cite{wagda}. In these settings, where data is contributed by multiple sources and get integrated into large repositories, data errors and inconsistencies frequently occur. Errors can lead scientists to wrong conclusions, but cleaning these datasets and consolidating conflicts is a tedious process. A framework such as data debugging will be a valuable tool for scientists in all experimental disciplines, who rely on the correctness of their data to make advancements in their respective fields.


The PIs plan to contribute to the broader research community by releasing software, publishing papers, and getting involved in various community services. The PIs already have a significant track record in this area, and they regularly participate in conference and workshop organizations. The data debugging project should offer similar opportunities for the PIs to contribute to the research community.
% paragraph scientific (end)

\todo{Economic and other impact.} If successful, this project will dramatically
reduce the risks of human data entry errors or data corruption,
increase the reliability of computations over data, and potentially
save the US economy millions of dollars.

The investigators will make their benchmarks and tools publicly
available, adding to the national research infrastructure. Several of
the investigators' prior tools and systems are widely used in research
and industry, including the Hoard high-performance memory manager, and the
DieHard error-tolerant and DieHarder secure runtime systems now
incorporated in Windows. Educational impact will include training
graduate and undergraduate students, contributing to the technology
workforce, and outreach to under-represented groups via the inclusion
of female students from nearby Mount Holyoke and Smith Colleges.

The PIs have substantial expertise in static analysis, dynamic
analysis, statistical analysis techniques, and optimizations in both
programming language and relational database settings, making them
extremely well-qualified to carry out this research.


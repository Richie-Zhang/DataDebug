\section{Medium-Scale Data Debugging: Relational Databases} % (fold)
\label{sec:bi_directional}

We now describe how we propose to apply data debugging techniques to
perform data cleaning on relational data.  Cleaning tools attempt to
purge datasets of discrepancies before the data can be used, but many
errors still go undetected and get propagated further through queries
and other transformations. When errors are detected in transformation
results, it is critical to trace them back to their source and correct
them to prevent them from affecting other computations. Data debugging
can facilitate \emph{post factum} cleaning by identifying the input
data with the highest contributions to an incorrect result. To make
this efficient, we propose to combine two methods of impact analysis
in large-scale relational datasets: forward and reverse analysis.

\subsection{Forward Analysis} % (fold)
\label{sub:forward_analysis}

The approach outlined in Section~\ref{sec:overview} relies on
re-calculating computations either exhaustively, or using random
sampling over a range of data. In a database setting, this approach
can be viewed as an extreme form of \emph{forward analysis}: an
analysis that starts with data and observes its effect on a
query. Previous forward analyses in database research include
\emph{hypothetical}, or \emph{what-if} queries
\cite{DBLP:conf/vldb/BalminPP00,DBLP:conf/icde/LakshmananRS08}: a
query result is computed based on a (single) hypothetical change to
the input data. Data debugging requires invoking multiple what-if
queries to find values with inordinately high impacts.

Forward analysis is powerful because it does not require any knowledge
of the inner workings of the computation, but for database systems,
directly adopting the data debugging approach outlined earlier would
be inefficient. Computing the transformation itself may be
computationally intensive, and calculating the exact impact of each
input tuple would require an exhaustive search over the input domain.

\subsection{Reverse Analysis} % (fold)
\label{sub:reverse_analysis}
Forward impact analysis is also not well-suited for certain errors that
result from data integration. To address these cases, we propose
\emph{reverse analysis}, a technique that analyzes transformations,
such as SQL queries, and virtually inverts them to derive an input
from a given output.

Data in databases often comes from multiple sources, and is collected
via multiple methods. In practice, databases are often produced by
merging other existing databases, and this process can introduce
redundancies and inconsistencies. Data integration errors are often
identified by the violation of key constraints and other functional
dependencies. Figure~\ref{fig:integrationExample} demonstrates a
key-constraint violation after merging two relations. A database needs
to satisfy several often complex constraints, and the cleaning task
may specify additional constraints or objectives.

\begin{figure}
	\small{\begin{tabular}{|l|l|l|l|l|l|l|l|}
		\multicolumn{5}{c}{Input tables} & \multicolumn{1}{l}{}& \multicolumn{2}{c}{Integration result}\\
		\multicolumn{5}{c}{$\overbrace{\rule{9.5cm}{0pt}}$} & \multicolumn{1}{l}{}& \multicolumn{2}{c}{$\overbrace{\rule{4.5cm}{0pt}}$}\\
		\multicolumn{2}{l}{Products (A)} & \multicolumn{1}{l}{} & \multicolumn{2}{l}{Products (B)} & \multicolumn{1}{l}{} & \multicolumn{2}{l}{All Products}\\
		\cline{1-2}\cline{4-5}\cline{7-8}
		\textbf{\underline{ItemNum}} & \textbf{product name} & \multicolumn{1}{l|}{} & \textbf{\underline{ItemNum}} & \textbf{product name} & \multicolumn{1}{l|}{} & \textbf{ItemNum} & \textbf{product name}\\
		\cline{1-2}\cline{4-5}\cline{7-8}
		5253B002 & Nikon D4 SLR & \multicolumn{1}{l|}{} & 5253B002 & Nikon D4 digital & \multicolumn{1}{l|}{} & 5253B002 & Nikon D4 SLR\\
		\cline{1-2}\cline{4-5}\cline{7-8}
		25482 & Canon EOS 1D X & \multicolumn{1}{l|}{} & 25482 & Canon 1DX EOS & \multicolumn{1}{l|}{} & 5253B002 & Nikon D4 digital\\
		\cline{1-2}\cline{4-5}\cline{7-8}
		\multicolumn{6}{l|}{} & 25482 & Canon EOS 1D X\\
		\cline{7-8}
		\multicolumn{6}{l|}{} & 25482 & Canon 1DX EOS\\
		\cline{7-8}
		
	\end{tabular}}
	\caption{Data integration errors: ItemNum is a key (its value is unique) in the input tables, but the same is not true for the integration result. The integrated data needs to be cleaned to produce a valid data instance.}\label{fig:integrationExample}
\end{figure}

Consider the following example of a data cleaning scenario that is not
well suited for forward analysis:

\begin{example}\label{ex:how-to}
A manufacturing company orders parts from multiple suppliers around
the world. To reduce its dependence from any single country, the
company makes sure that its inventory orders from each country do not
exceed 10\% of all orders. However, after merging with another
manufacturer, this condition is no longer satisfied. The company needs
to ``clean'' its inventory orders by reassigning some orders to
different suppliers. Ideally, it would like to achieve this with the
minimum number of changes.
\end{example}

In the presence of such complex conditions, forward impact analysis
would have to exhaustively search all possible scenarios, which is
intractable.

In previous work, we have modeled these scenarios with \emph{how-to}
queries: ``how should the input change in order to achieve the desired
output?''~\cite{DBLP:journals/pvldb/MeliouGS11} How-to queries are an
example of reverse analysis. In contrast to forward analysis,
reverse analysis specifies the desired effect on the output, and
reverse engineers how the input should change. Reverse analysis has
more complex semantics and higher implementation complexity compared
to forward analysis due to the fact the inverse of a function is not
always a function. Given a desired output, there may be multiple input
values (or none at all) that produce it.  As opposed to forward
analysis, reverse analysis cannot handle black box computations, but
uses knowledge of the transformations to perform the evaluation
efficiently.

%To circumvent this difficulty in a general framework we will use SAT and Mixed Integer Programming (MIP) solvers as general purpose tools in reverse analysis. 

% We propose to augment data debugging with \emph{reverse impact analysis}. In contrast to forward analysis (``how would the output change for a given change in the input''), reverse analysis answers the question ``how should the input change in order to achieve the desired output?'' 

The Tiresias system~\cite{DBLP:conf/sigmod/MeliouS12,
  DBLP:conf/sigmod/MeliouSS12}, developed by PI Meliou, can
efficiently evaluate how-to queries and perform complex cleaning
tasks such as the one in Example~\ref{ex:how-to}. How-to queries are
constraint optimization problems defined over relational data, but
they cannot be expressed in standard SQL. Tiresias allows users to
issue how-to queries through a declarative language based on
Datalog~\cite{DBLP:journals/tkde/CeriGT89}, and the queries are
evaluated using mixed integer programming solvers.

\paragraph{Calculating impact.} % (fold)
\label{par:calculating_impact}
Tiresias does not quantify the impact of input data, but rather
produces an instance of the input that satisfies all cleaning
constraints. In prior work, PI Meliou introduced a theory of causality
for relational databases~\cite{DBLP:journals/debu/MeliouGHKMS10},
which can model impact under the reverse analysis model. It is
possible to characterize the family of conjunctive queries for which
\emph{responsibility}---the contribution of an input to a result---can
be computed in polynomial
time~\cite{DBLP:journals/pvldb/MeliouGMS11}. Tiresias uses SAT solvers
to compute responsibility efficiently for NP-hard
cases~\cite{DBLP:conf/sigmod/MeliouGNS11}.


\subsection{Proposed Work} % (fold)
\label{sub:research_plan}

Complex transformations contain both glass box and black box
components. We propose to develop a \textbf{bi-directional data
  cleaning} framework that combines both forward and reverse
analysis. This framework can flexibly adapt to different types of
computations. We will investigate the following research questions:

\paragraph{Static analysis of SQL queries.} % (fold)
\label{par:static_analysis_of_sql_queries}
One way to reduce the cost of data debugging in databases is to peel
back the black box abstraction, as described in our proposed work for
spreadsheets. The black box abstraction makes it particularly
suitable for user-defined functions (UDF). However, many computations
in the relational model rely on SQL transformations, which are simple
to analyze.

Consider the following example SQL query, where the inner subquery
returns all departments that have a branch in Boston, and the outer
query computes the average salary of the employees that do not work in
any of those departments.
\texttt{
\begin{tabbing}
		SELECT	\=avg(E.salary)\\
		FROM	\>Employees E\\
		WHERE	\>E.department NOT IN	(\=SELECT	\=distinct D.name\+\+\\
										FROM	\>Department D, Branches B\\
										WHERE	\>D.did = B.did AND B.location = `Boston')
\end{tabbing}}

\noindent
In this example, we do not need to recompute the nested subquery if
only the \texttt{Employees} table is modified during impact
analysis. SQL queries are particularly amenable to such optimizations.
% paragraph static_analysis_of_sql_queries (end)

\paragraph{SQL rewrites.} % (fold)
\label{par:sql_rewrites}
SQL is a declarative language, so a query can be expressed in many
different forms, each producing the same result, but possibly having
vastly different execution plans and performance. 
%The problem of query
%equivalence is undecidable in general, and it is even NP-hard for
%conjunctive
%queries~\cite{abiteboul1994foundations,DBLP:conf/pods/CalvaneseGL98}. However,
%there are several heuristics that guide query rewrites, e.g.,
%unnesting and
%decorrelation~\cite{DBLP:conf/sigmod/PiraheshHH92}. 
Traditional query rewriting is geared toward improving query
runtime~\cite{DBLP:conf/sigmod/PiraheshHH92}, but data debugging may
often have conflicting objectives, e.g., it favors some types of
nesting. We will investigate heuristics for query rewriting, with the
objective of optimizing repeated execution in the context of data
debugging.
% paragraph sql_rewrites (end)

\punt{
We will also investigate techniques to break down queries into
\emph{active} and \emph{inactive} components: active components are
the parts of the query that need to be recomputed during forward
analysis, whereas inactive are components that compute intermediate
results that do not change and thus can be precomputed. While it is
not possible to do this for all queries, as it relates to the
undecidable problems of query containment and
equivalence~\cite{abiteboul1994foundations,DBLP:conf/pods/CalvaneseGL98},
we will do \comment{what exactly?}

}

\paragraph{Cleaning optimizations.} % (fold)
\label{par:cleaning_optimizations}
In complex transformations, it is not clear how forward and reverse analysis
should be combined. We will use computational patterns to design a cleaning
optimizer, modeled after traditional relational query optimizers. Given a
cleaning task, the optimizer should select how to subdivide the task into the
forward and reverse components, and in which order the tasks should be
performed.
% paragraph cleaning_optimizations (end)

\paragraph{Interactive cleaning.} % (fold)
\label{par:interactive_cleaning}
Cleaning tasks are often based on complex constraints, and have many possible
solutions. It can be a daunting undertaking for a user to assess these
solutions, or even define the cleaning task itself. We will design an
interactive interface to allow users to easily identify possible problems, and
we will investigate techniques to refine solutions based on user feedback.
% paragraph interactive_cleaning (end)



% subsection research_plan (end)



% section bi_directional (end)
